library(tidyverse)
library(tidytext)
library(SnowballC)
data <- read_tsv("D:R/AHDS_assessment_code_2600216/clean/article_info.tsv")
write_tsv(final_titles, data <- read_tsv("D:R/AHDS_assessment_code_2600216/clean/cleaned_article_info.tsv"))
head(final_titles)
cleaned_titles <- cleaned_titles %>%
mutate(stemmed_word = wordStem(word))
data <- read_tsv("D:R/AHDS_assessment_code_2600216/clean/article_info.tsv")
data <- read_tsv("D:R/AHDS_assessment_code_2600216/clean/article_info.tsv")
data <- read_tsv("D:/R/AHDS_assessment_code_2600216/clean/article_info.tsv")
cleaned_titles <- data %>%
unnest_tokens(word, Title) %>%
filter(!word %in% stop_words$word) %>%
filter(!str_detect(word, "\\d"))
cleaned_titles <- cleaned_titles %>%
mutate(stemmed_word = wordStem(word))
final_titles <- cleaned_titles %>%
group_by(PMID, Year) %>%
summarize(cleaned_title = paste(stemmed_word, collapse = " "), .groups = "drop")
head(final_titles)
write_tsv(final_titles, data <- read_tsv("D:/R/AHDS_assessment_code_2600216/clean/cleaned_article_info.tsv"))
data <- read_tsv("D:/R/AHDS_assessment_code_2600216/clean/cleaned_article_info.tsv")
word_freq <- data %>%
unnest_tokens(word, cleaned_title) %>%
count(Year, word, sort = TRUE) %>%
group_by(Year) %>%
top_n(10, n)
ggplot(word_freq, aes(x = Year, y = n, fill = word)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
fill = "Word")
ggplot(word_freq, aes(x = Year, y = n, color = word, group = word)) +
geom_line(size = 1) +                   # 绘制折线
geom_point(size = 2) +                  # 添加点以突出数据
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
color = "Word") +
theme(legend.position = "bottom")
word_freq <- data %>%
unnest_tokens(word, cleaned_title) %>%  # 拆分文章标题为单词
count(Year, word, sort = TRUE) %>%      # 统计每年单词频率
group_by(Year) %>%
slice_max(n, n = 10)                    # 每年取频率最高的 10 个词
ggplot(word_freq, aes(x = Year, y = n, color = word, group = word)) +
geom_line(size = 1) +                   # 绘制折线
geom_point(size = 2) +                  # 添加点以突出数据
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
color = "Word") +
theme(legend.position = "bottom")
ggplot(word_freq, aes(x = Year, y = n, fill = word)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
fill = "Word")
data <- read_tsv("D:/R/AHDS_assessment_code_2600216/clean/cleaned_article_info.tsv")
# 处理单词频率
word_freq <- data %>%
unnest_tokens(word, cleaned_title) %>%  # 拆分文章标题为单词
count(Year, word, sort = TRUE) %>%      # 统计每年单词频率
group_by(Year) %>%
slice_max(n, n = 10)                    # 每年取频率最高的 10 个词
# 绘制折线图
ggplot(word_freq, aes(x = Year, y = n, color = word, group = word)) +
geom_line(size = 1) +                   # 绘制折线
geom_point(size = 2) +                  # 添加点以突出数据
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
color = "Word") +
theme(legend.position = "bottom")       # 将图例放在底部
word_freq <- data %>%
unnest_tokens(word, cleaned_title) %>%
count(Year, word, sort = TRUE) %>%
group_by(Year) %>%
top_n(10, n)
ggplot(word_freq, aes(x = Year, y = n, fill = word)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
fill = "Word")
# 处理单词频率
word_freq <- data %>%
unnest_tokens(word, cleaned_title) %>%  # 拆分文章标题为单词
count(Year, word, sort = TRUE) %>%      # 统计每年单词频率
group_by(Year) %>%
slice_max(n, n = 10)                    # 每年取频率最高的 10 个词
# 确保 Year 是数值型
word_freq <- word_freq %>%
mutate(Year = as.numeric(Year))
# 绘制折线图
ggplot(word_freq, aes(x = Year, y = n, color = word, group = word)) +
geom_line(size = 1) +                   # 添加折线
geom_point(size = 2) +                  # 添加数据点
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
color = "Word") +
theme(legend.position = "bottom")       # 图例放置在底部
word_freq <- data %>%
unnest_tokens(word, cleaned_title) %>%
count(Year, word, sort = TRUE) %>%
group_by(Year) %>%
top_n(10, n)
ggplot(word_freq, aes(x = Year, y = n, fill = word)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
fill = "Word")
str(word_freq$Year)
table(word_freq$Year)
# 处理单词频率
word_freq <- data %>%
unnest_tokens(word, cleaned_title) %>%  # 拆分文章标题为单词
count(Year, word, sort = TRUE) %>%      # 统计每年单词频率
group_by(Year) %>%
slice_max(n, n = 10)                    # 每年取频率最高的 10 个词
# 确保 Year 是数值型
word_freq <- word_freq %>%
mutate(Year = as.numeric(Year))  # 如果是字符型年份，强制转换为数值
# 绘制折线图
ggplot(word_freq, aes(x = Year, y = n, color = word, group = word)) +
geom_line(size = 1) +                   # 绘制折线
geom_point(size = 2) +                  # 添加数据点
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
color = "Word") +
theme(legend.position = "bottom")       # 将图例放在底部
library(tidyverse)
library(tidytext)
# 确保 Year 是数值型，并补全年份数据
word_freq <- word_freq %>%
mutate(Year = as.numeric(Year)) %>%  # 确保 Year 为数值型
complete(Year, word, fill = list(n = 0))  # 补全缺失年份的数据，频率设为 0
# 绘制折线图
ggplot(word_freq, aes(x = Year, y = n, color = word, group = word)) +
geom_line(size = 1) +                   # 添加折线
geom_point(size = 2) +                  # 添加数据点
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
color = "Word") +
theme(legend.position = "bottom")       # 将图例放置在底部
data <- read_tsv("D:/R/AHDS_assessment_code_2600216/clean/cleaned_article_info.tsv")
word_freq <- data %>%
unnest_tokens(word, cleaned_title) %>%
count(Year, word, sort = TRUE) %>%
group_by(Year) %>%
top_n(10, n)
ggplot(word_freq, aes(x = Year, y = n, fill = word)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
fill = "Word")
head(word_freq)
# 补全数据，确保每个单词在所有年份都有记录
word_freq <- word_freq %>%
complete(Year = full_seq(Year, 1), word, fill = list(n = 0))  # 补全年份数据，频率为 0
word_freq <- data %>%
unnest_tokens(word, cleaned_title) %>%
count(Year, word, sort = TRUE) %>%
group_by(Year) %>%
top_n(10, n)
library(tidyverse)
library(tidytext)
# 补全数据，确保每个单词在所有年份都有记录
word_freq <- word_freq %>%
complete(Year = full_seq(Year, 1), word, fill = list(n = 0))  # 补全年份数据，频率为 0
# 绘制折线图
ggplot(word_freq, aes(x = Year, y = n, color = word, group = word)) +
geom_line(size = 1) +                   # 绘制折线
geom_point(size = 2) +                  # 添加数据点
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
color = "Word") +
theme(legend.position = "bottom")       # 图例放置在底部
data <- read_tsv("D:/R/AHDS_assessment_code_2600216/clean/cleaned_article_info.tsv")
word_freq <- data %>%
unnest_tokens(word, cleaned_title) %>%
count(Year, word, sort = TRUE) %>%
group_by(Year) %>%
top_n(10, n)
ggplot(word_freq, aes(x = Year, y = n, fill = word)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
fill = "Word")
# 筛选 3-5 个关键单词
key_words <- c("covid", "disease", "research")
word_freq_filtered <- word_freq %>%
filter(word %in% key_words)
# 绘制折线图
ggplot(word_freq_filtered, aes(x = Year, y = n, color = word, group = word)) +
geom_line(size = 1) +
geom_point(size = 2) +
theme_minimal() +
labs(title = "Key Word Frequency Over Time",
x = "Year",
y = "Frequency",
color = "Word") +
theme(legend.position = "bottom")
ggplot(word_freq, aes(x = Year, y = n, fill = word)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
fill = "Word")
output_path <- "D:/R/AHDS_assessment_code_2600216/plot/word_frequency_plot.png"
# ggsave
ggsave(output_path,
plot = last_plot(),
width = 10,
height = 6,
dpi = 300)
getwd()
data <- read_tsv("clean/article_info.tsv")
getwd()
getwd()
data <- read_tsv("../clean/article_info.tsv")
cleaned_titles <- data %>%
unnest_tokens(word, Title) %>%
filter(!word %in% stop_words$word) %>%
filter(!str_detect(word, "\\d"))
cleaned_titles <- cleaned_titles %>%
mutate(stemmed_word = wordStem(word))
final_titles <- cleaned_titles %>%
group_by(PMID, Year) %>%
summarize(cleaned_title = paste(stemmed_word, collapse = " "), .groups = "drop")
head(final_titles)
write.tsv(final_titles, data <- read_tsv("../clean/cleaned_article_info.tsv"))
library(tidyverse)
write.tsv(final_titles, data <- read_tsv("../clean/cleaned_article_info.tsv"))
data <- read_tsv("../clean/article_info.tsv")
cleaned_titles <- data %>%
unnest_tokens(word, Title) %>%
filter(!word %in% stop_words$word) %>%
filter(!str_detect(word, "\\d"))
cleaned_titles <- cleaned_titles %>%
mutate(stemmed_word = wordStem(word))
final_titles <- cleaned_titles %>%
group_by(PMID, Year) %>%
summarize(cleaned_title = paste(stemmed_word, collapse = " "), .groups = "drop")
head(final_titles)
write_tsv(final_titles, data <- read_tsv("../clean/cleaned_article_info.tsv"))
data <- read_tsv("../clean/article_info.tsv")
cleaned_titles <- data %>%
unnest_tokens(word, Title) %>%
filter(!word %in% stop_words$word) %>%
filter(!str_detect(word, "\\d"))
cleaned_titles <- cleaned_titles %>%
mutate(stemmed_word = wordStem(word))
final_titles <- cleaned_titles %>%
group_by(PMID, Year) %>%
summarize(cleaned_title = paste(stemmed_word, collapse = " "), .groups = "drop")
head(final_titles)
write_tsv(final_titles, data <- read_tsv("../clean/cleaned_article_info.tsv"))
getwd()
data <- read_tsv("../clean/article_info.tsv")
write_tsv(data, "../clean/test_output.tsv")
write_tsv(data, "../clean/test_output.tsv")
write_tsv(data, "../clean/test_output.tsv")
write_tsv(final_titles, data <- read_tsv("../clean/cleaned_article_info.tsv"))
data <- read_tsv("../clean/cleaned_article_info.tsv")
library(tidyverse)
library(tidytext)
library(SnowballC)
data <- read_tsv("../clean/article_info.tsv")
cleaned_titles <- data %>%
unnest_tokens(word, Title) %>%
filter(!word %in% stop_words$word) %>%
filter(!str_detect(word, "\\d"))
cleaned_titles <- cleaned_titles %>%
mutate(stemmed_word = wordStem(word))
final_titles <- cleaned_titles %>%
group_by(PMID, Year) %>%
summarize(cleaned_title = paste(stemmed_word, collapse = " "), .groups = "drop")
head(final_titles)
write_tsv(final_titles, data <- read_tsv("../clean/cleaned_article_info.tsv"))
write_tsv(data, "../clean/test_output.tsv")
write_tsv(final_titles, "../clean/cleaned_article_info.tsv")
library(tidyverse)
library(tidytext)
library(SnowballC)
data <- read_tsv("../clean/article_info.tsv")
cleaned_titles <- data %>%
unnest_tokens(word, Title) %>%
filter(!word %in% stop_words$word) %>%
filter(!str_detect(word, "\\d"))
cleaned_titles <- cleaned_titles %>%
mutate(stemmed_word = wordStem(word))
final_titles <- cleaned_titles %>%
group_by(PMID, Year) %>%
summarize(cleaned_title = paste(stemmed_word, collapse = " "), .groups = "drop")
head(final_titles)
write_tsv(final_titles, "../clean/cleaned_article_info.tsv")
data <- read_tsv("../clean/cleaned_article_info.tsv")
word_freq <- data %>%
unnest_tokens(word, cleaned_title) %>%
count(Year, word, sort = TRUE) %>%
group_by(Year) %>%
top_n(10, n)
ggplot(word_freq, aes(x = Year, y = n, fill = word)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
fill = "Word")
output_path <- "plot/word_frequency_plot.png"
# ggsave
ggsave(output_path,
plot = last_plot(),
width = 10,
height = 6,
dpi = 300)
data <- read_tsv("../clean/cleaned_article_info.tsv")
word_freq <- data %>%
unnest_tokens(word, cleaned_title) %>%
count(Year, word, sort = TRUE) %>%
group_by(Year) %>%
top_n(10, n)
ggplot(word_freq, aes(x = Year, y = n, fill = word)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
labs(title = "Word Frequency Over Time",
x = "Year",
y = "Frequency",
fill = "Word")
output_path <- "plot/word_frequency_plot.png"
# ggsave
ggsave(output_path,
plot = last_plot(),
width = 10,
height = 6,
dpi = 300)
data <- read_tsv("../clean/article_info.tsv")
cleaned_titles <- data %>%
unnest_tokens(word, Title) %>%
filter(!word %in% stop_words$word) %>%
filter(!str_detect(word, "\\d"))
cleaned_titles <- cleaned_titles %>%
mutate(stemmed_word = wordStem(word))
final_titles <- cleaned_titles %>%
group_by(PMID, Year) %>%
summarize(cleaned_title = paste(stemmed_word, collapse = " "), .groups = "drop")
head(final_titles)
write_tsv(final_titles, "../clean/cleaned_article_info.tsv")
data <- read_tsv("../clean/article_info.tsv")
cleaned_titles <- data %>%
unnest_tokens(word, Title) %>%
filter(!word %in% stop_words$word) %>%
filter(!str_detect(word, "\\d"))
cleaned_titles <- cleaned_titles %>%
mutate(stemmed_word = wordStem(word))
final_titles <- cleaned_titles %>%
group_by(PMID, Year) %>%
summarize(cleaned_title = paste(stemmed_word, collapse = " "), .groups = "drop")
head(final_titles)
write_tsv(final_titles, "../clean/cleaned_article_info.tsv")
data <- read_tsv("../clean/article_info.tsv")
cleaned_titles <- data %>%
unnest_tokens(word, Title) %>%
filter(!word %in% stop_words$word) %>%
filter(!str_detect(word, "\\d"))
cleaned_titles <- cleaned_titles %>%
mutate(stemmed_word = wordStem(word))
final_titles <- cleaned_titles %>%
group_by(PMID, Year) %>%
summarize(cleaned_title = paste(stemmed_word, collapse = " "), .groups = "drop")
head(final_titles)
write_tsv(final_titles, "../clean/cleaned_article_info.tsv")
print(data)
write_tsv(data, "clean/test_output.tsv")
write_tsv(data, "../clean/test_output.tsv")
